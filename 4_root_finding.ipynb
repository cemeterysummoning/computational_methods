{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources of error\n",
    "1. There aren't exact representations for decimal numbers in a binary number system.\n",
    "2. Floating point representation is often imprecise. We have the idea of machine epsilon, which measures this disparity as numbers become larger in magnitude.\n",
    "3. We have a fixed amount of memory in computation. We cannot have a number that requires a quantity of bits that exceeds the memory storage of our machine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Root finding\n",
    "Given a continuous function $f(x)$, the process of root finding or root approximation involves finding some $x$ such that $f(x) = 0$. \n",
    "If a mathematician gives this problem to a computer, the computer can fist ask for an interval $[x_1, x_2]$, where \n",
    "1. $f(x_1) == 0$ or $f(x_2) == 0$\n",
    "2. $f(x_1) \\cdot f(x_2) < 0$\n",
    "\n",
    "Ideally, $x_1$ and $x_2$ will be \"close together\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bisection Method\n",
    "We start with an interval $I = [x_1, x_2]$, where $f(x_1) \\cdot f(x_2) < 0$.\n",
    "By the intermediate value theorem from analysis, we know that a zero must exist inside the interval $I$. \n",
    "We can first compute and check: $f\\left( \\frac{x_1 + x_2}{2} \\right) \\stackrel{?}{=} 0$\n",
    "1. If $f\\left( \\frac{x_1 + x_2}{2} \\right)$ has the same sign as $f(x_1)$, then we must \"move to the right\": our new interval is now $\\left[\\frac{x_1 + x_2}{2}, x_2 \\right]$\n",
    "2. If $f\\left( \\frac{x_1 + x_2}{2} \\right)$ has the same sign as $f(x_2)$, then we must \"move to the left\": our new interval is now $\\left[x_1, \\frac{x_1 + x_2}{2}\\right]$\n",
    "\n",
    "We repeat this process until we are satisfied with our results: either we find a real zero, or $x_1$ and $x_2$ become reasonably close enough together. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major drawback to the bisection method is that it is extremely slow; although it is guaranteed to converge, it may take a huge number of iterations before it does so. We introduce another approximation technique:\n",
    "# Newton-Raphson Method\n",
    "Newton's method is significantly faster and more efficient than the Bisection method. However, it is not guaranteed to converge: there are many edge cases in which the method reaches an \"oscillatory\" interval, which will alternate between two values for eternity. We need to choose a good \"guess\" in order to utilize the Newton-Raphson method effectively. For Newton's method to converge:\n",
    "1. We must be able to take the derivative of the function.\n",
    "2. The derivative must be continuous at the root.\n",
    "3. The initial guess must be at a \"good\" point - it should not lead to undefined quantities or oscillations.\n",
    "\n",
    "The algorithm works as follows:\n",
    "1. Pick an initial point as a \"guess\"\n",
    "2. Check: is this guess our root? If it's not, move to step 3\n",
    "3. Construct the tangent line of $f(x)$ at $x_0$\n",
    "4. Use the x-intercept of the tangent line as the new guess, $x_1$. \n",
    "$$x_1 = x_0 - \\frac{f(x_0)}{f'(x_0)}$$\n",
    "$$x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}$$\n",
    "5. Iterate until a certain stopping condition is met: the root has been found, or the program terminates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newton (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function newton(f, f_prime, x, iterations)\n",
    "    if iterations > 0\n",
    "        for i in 1:iterations\n",
    "            if f(x) == 0\n",
    "                return x\n",
    "            end\n",
    "            x = x - f(x) / f_prime(x)\n",
    "        end\n",
    "        return x, iterations\n",
    "    else\n",
    "        x_prev = 0\n",
    "        count = 0\n",
    "        while abs(x - x_prev) >= eps(Float32)\n",
    "            x_prev = x\n",
    "            x = x - f(x) / f_prime(x)\n",
    "            count += 1\n",
    "        end\n",
    "        return x, count\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.678573510428327, 4)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = x -> x^3 - 5x^2 + 3x - 7\n",
    "f_prime = x -> 3x^2 - 10x + 3 \n",
    "x_0 = 5\n",
    "newton(f, f_prime, x_0, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.1415947167395113, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = x -> tan(x)\n",
    "g_prime = x -> (sec(x))^2\n",
    "x_0 = 4.5\n",
    "newton(g, g_prime, x_0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.283615067811668, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_0 = 7.7\n",
    "newton(g, g_prime, x_0, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rates of Convergence\n",
    "How do we determine how quickly we arrive at a solution when using a root finding algorithm?\n",
    "\n",
    "Suppose we have a sequence of numbers $x_i$ that converges to some solution $r$. \n",
    "$$x_1, x_2, x_3, \\cdots , x_n \\to r$$\n",
    "\n",
    "The rate of convergence of this sequence is defined to be \n",
    "$$\\lim_{n \\to \\infty} \\frac{|x_{n + 1} - r|}{|x_n - r|^\\alpha}$$\n",
    "where $\\alpha$ is some arbitrary power.\n",
    "\n",
    "If the limit converges to some $\\lambda$ less than $\\infty$, then $\\alpha$ is what is called our *rate of convergence*.\n",
    "\n",
    "Generally, \n",
    "$$1 \\leq \\alpha \\leq 2$$\n",
    "If $\\alpha = 1$, we call this linear convergence. \n",
    "\n",
    "If $\\alpha = 2$, we call this quadratic convergence. \n",
    "\n",
    "If $1 < \\alpha < 2$, we call this superlinear convergence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we plug Newton's method into the formula for the rate of convergence, we arrive at the limit \n",
    "$$\\lim_{n \\to \\infty} \\frac{|x_{n + 1} - r|}{|x_n - r|} = |g'(r)|$$\n",
    "Notice that this method has linear convergence. If Newton's method fulfills some specific conditions, we can turn this into\n",
    "$$\\lim_{n \\to \\infty} \\frac{|x_{n + 1} - r|}{|x_n - r|^2} = \\frac{|g''(r)|}{2}$$\n",
    "Quadratic convergence, shown in the limit above, is highly desirable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
